{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mZhmra3mcz6"
   },
   "source": [
    "## Exercise - DL Tutorial 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrSPQusomcz-"
   },
   "source": [
    "## student name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nStp1HcKmc0T"
   },
   "outputs": [],
   "source": [
    "# Equation numbers refer to handout 5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1 +np.exp(-X))\n",
    "\n",
    "def del_sigmoid(h):\n",
    "    return h * (1 - h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSBUWqkemc0Y"
   },
   "source": [
    "Implement a method that creates binary addition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcQvj8Xmmc0Z"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_examples, max_len):\n",
    "    # Generate num_examples * 2 ints.\n",
    "    rand_numbers = np.random.randint(0, 2**(max_len-1)-1, size=(num_examples * 2), dtype=np.uint8)\n",
    "    rand_numbers_bits = np.unpackbits(rand_numbers)\n",
    "    rand_numbers = rand_numbers.reshape(num_examples, 2)\n",
    "    rand_numbers_bits = rand_numbers_bits.reshape(num_examples, 2, max_len)\n",
    "    rand_results = np.sum(rand_numbers, axis=1, dtype=np.uint8)\n",
    "    # Add 3rd dimension to tensor.\n",
    "    rand_results_bits = np.unpackbits(rand_results).reshape(num_examples, max_len, 1)\n",
    "    # Data should be of form (num_examples, sequence_length, num_features).\n",
    "    rand_numbers_bits = np.transpose(rand_numbers_bits, axes=[0,2,1])\n",
    "\n",
    "    return rand_numbers_bits, rand_results_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "[0 0 0 1 1 0 1 0]\n",
      "+\n",
      "[0 0 0 0 1 0 1 1]\n",
      "=\n",
      "[0 0 1 0 0 1 0 1]\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = generate_data(100, 8)\n",
    "print(\"-----------------------------------\")\n",
    "print(trainX[0,:,0])\n",
    "print(\"+\")\n",
    "print(trainX[0,:,1])\n",
    "print(\"=\")\n",
    "print(trainY[0,:,0])\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cu4-nHGYmc0d"
   },
   "source": [
    "Implement the mean squared error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Swdz9olbmc0e"
   },
   "outputs": [],
   "source": [
    "def mean_square_error(pred, y):\n",
    "    return np.mean((pred-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-6Wz94Amc0j"
   },
   "source": [
    "Iimplement the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KFYwvuFmc0k"
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    rounded = np.rint(pred)\n",
    "    return np.mean(rounded==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxdJMootmc0o"
   },
   "source": [
    "Implement the RNN class, implement the forward propagation, implement the BPTT and implement the gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbV0BbyFmc0q"
   },
   "outputs": [],
   "source": [
    "class one_layer_rnn:\n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        # Initialisation of weights, no bias.\n",
    "        self.W_1 = np.random.randn(n_input, n_hidden)\n",
    "        self.U = np.random.randn(n_hidden, n_hidden)\n",
    "        self.W_2 = np.random.randn(n_hidden, n_out)\n",
    "        self.X = None\n",
    "        self.H = None\n",
    "        self.out = None\n",
    "        self.dW1 = None\n",
    "        self.db_h1 = None\n",
    "        self.dU = None\n",
    "        self.dW2 = None\n",
    "        self.db_out = None\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        num_sequence = X.shape[1]\n",
    "        # Reverse sequence order to process bits from low-valued to high-valued\n",
    "        self.X = np.flip(X, axis=1)\n",
    "        \n",
    "        # \"dot\" multiplication of X and W_1 is performed over the last dimension of X \n",
    "        # (the features for one sequence and one training example) and W_1. \n",
    "        # Result: H without any horizontal information flow.\n",
    "        # (2)\n",
    "        self.H = np.dot(self.X, self.W_1)\n",
    "        prev = np.zeros(self.H[:, 0, :].shape)\n",
    "        # Loop over sequence. Numbers have to be added (after reversal) from left to right\n",
    "        for i in range(num_sequence):\n",
    "            # Matrix multiplication of ith element of sequence. Adding of horizontal information flow.\n",
    "            # (2)\n",
    "            self.H[:, i, :] = sigmoid(self.H[:, i, :] + np.dot(prev, self.U))\n",
    "            prev = self.H[:, i, :]\n",
    "        # (3)\n",
    " \n",
    "        self.out = sigmoid(np.dot(self.H, self.W_2))\n",
    "        \n",
    "        # Reverse sequence back to the original order\n",
    "        return np.flip(self.out, axis=1)\n",
    "\n",
    "    def backprop_through_time(self, Y):\n",
    "        num_examples, num_sequence = self.X.shape[:2]\n",
    "        \n",
    "        # Derivative of mean-square error see (6).\n",
    "        # Note: target labels are flipped along the sequence axis to match the sequence reversal.\n",
    "        self.d_out = 2 * (self.out - np.flip(Y, axis=1)) * del_sigmoid(self.out)\n",
    "        \n",
    "        \n",
    "        self.dW2 = np.zeros(self.W_2.shape)\n",
    "        # Backprop: left to right.\n",
    "        for i in range(num_sequence):\n",
    "            # Sum up contribution of all sequence results to dW2.\n",
    "            # Basically, sum over (5), like in (14).\n",
    "            #self.dW2 += (self.d_out[:, i, :].T @ self.H[:, i, :]).T\n",
    "            self.dW2 += self.H[:, i, :].T @ self.d_out[:, i, :]\n",
    "        # Average gradient for every sequence element.\n",
    "        self.dW2 /= num_examples\n",
    "        \n",
    "        \n",
    "            \n",
    "        # (13): W^n \\delta^{n,\\tau} part, vertical backprop.\n",
    "        self.d_hidden = np.dot(self.d_out, self.W_2.T)\n",
    "        prev = np.zeros(self.d_hidden[:, 0, :].shape)\n",
    "        # backpropagation trhough time\n",
    "        for i in range(num_sequence - 1, -1, -1):\n",
    "            # (13): U^{n-1} \\delta^{n-1,\\tau + 1} part, horizontal backprop.\n",
    "            self.d_hidden[:, i, :] += prev @ self.U.T\n",
    "            self.d_hidden[:, i, :] *= del_sigmoid(self.H[:, i, :])\n",
    "            prev = self.d_hidden[:, i, :]\n",
    "        \n",
    "        \n",
    "        self.dW1 = np.zeros(self.W_1.shape)\n",
    "        for i in range(num_sequence):\n",
    "            # (13) only vertical backprop necessary.\n",
    "            self.dW1 += self.X[:, i, :].T @ self.d_hidden[:, i, :]\n",
    "        self.dW1/= num_examples\n",
    "        \n",
    "        \n",
    "        self.dU =  np.zeros(self.U.shape)\n",
    "        for i in range(1, num_sequence):\n",
    "            self.dU += self.H[:, i - 1, :].T @ self.d_hidden[:, i, :]\n",
    "        self.dU /= num_examples\n",
    "        \n",
    "\n",
    "    def gradient_step(self, learning_rate):\n",
    "        self.U -= learning_rate*self.dU\n",
    "        self.W_2 -= learning_rate * self.dW2\n",
    "        self.W_1 -= learning_rate * self.dW1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPgxqLskmc0u"
   },
   "source": [
    "Implement the learning routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRozmS-Dmc0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "[0 1 0 0 0 0 1 1]\n",
      "+\n",
      "[0 0 1 1 1 1 1 1]\n",
      "=\n",
      "[1 0 0 0 0 0 1 0]\n",
      "-----------------------------------\n",
      "Shapes\n",
      "X        shape: (100, 8, 2)\n",
      "H        shape: (100, 8, 16)\n",
      "out      shape: (100, 8, 1)\n",
      "d_out    shape: (100, 8, 1)\n",
      "d_hidden shape: (100, 8, 16)\n",
      "dW2      shape: (16, 1)\n",
      "dW1      shape: (2, 16)\n",
      "dU       shape: (16, 16)\n",
      "-----------------------------------\n",
      "Iteration: 0\n",
      "Test loss: \t0.2591587365780634\n",
      "Test acc: \t0.50305\n",
      "Train loss: \t0.2588415805614865\n",
      "Train acc: \t0.5025\n",
      "Iteration: 100\n",
      "Test loss: \t0.24870291017759869\n",
      "Test acc: \t0.5259\n",
      "Train loss: \t0.2461162712411344\n",
      "Train acc: \t0.54875\n",
      "Iteration: 200\n",
      "Test loss: \t0.24494756383286723\n",
      "Test acc: \t0.57655\n",
      "Train loss: \t0.24089750093059656\n",
      "Train acc: \t0.605\n",
      "Iteration: 300\n",
      "Test loss: \t0.2411127881903506\n",
      "Test acc: \t0.582775\n",
      "Train loss: \t0.2356216443745122\n",
      "Train acc: \t0.62\n",
      "Iteration: 400\n",
      "Test loss: \t0.2362729685075586\n",
      "Test acc: \t0.6013625\n",
      "Train loss: \t0.2290455232504128\n",
      "Train acc: \t0.64\n",
      "Iteration: 500\n",
      "Test loss: \t0.230102042130465\n",
      "Test acc: \t0.627825\n",
      "Train loss: \t0.22078965341886897\n",
      "Train acc: \t0.69125\n",
      "Iteration: 600\n",
      "Test loss: \t0.2226532616316009\n",
      "Test acc: \t0.6681375\n",
      "Train loss: \t0.2111426918861716\n",
      "Train acc: \t0.72625\n",
      "Iteration: 700\n",
      "Test loss: \t0.2142244651085215\n",
      "Test acc: \t0.691075\n",
      "Train loss: \t0.2008164332685576\n",
      "Train acc: \t0.74125\n",
      "Iteration: 800\n",
      "Test loss: \t0.20498528869396182\n",
      "Test acc: \t0.7035\n",
      "Train loss: \t0.19010161217364194\n",
      "Train acc: \t0.76125\n",
      "Iteration: 900\n",
      "Test loss: \t0.19502877074919409\n",
      "Test acc: \t0.71555\n",
      "Train loss: \t0.17906839909180988\n",
      "Train acc: \t0.775\n",
      "Iteration: 1000\n",
      "Test loss: \t0.1847661102066876\n",
      "Test acc: \t0.7232375\n",
      "Train loss: \t0.1680242333400652\n",
      "Train acc: \t0.78\n",
      "Iteration: 1100\n",
      "Test loss: \t0.17450800948962664\n",
      "Test acc: \t0.7488625\n",
      "Train loss: \t0.15721828586777958\n",
      "Train acc: \t0.79125\n",
      "Iteration: 1200\n",
      "Test loss: \t0.1641842183558683\n",
      "Test acc: \t0.7528\n",
      "Train loss: \t0.14665650712351588\n",
      "Train acc: \t0.7925\n",
      "Iteration: 1300\n",
      "Test loss: \t0.1536174539264896\n",
      "Test acc: \t0.7733375\n",
      "Train loss: \t0.13624472316322384\n",
      "Train acc: \t0.80375\n",
      "Iteration: 1400\n",
      "Test loss: \t0.14279130877835233\n",
      "Test acc: \t0.797475\n",
      "Train loss: \t0.12592323725160415\n",
      "Train acc: \t0.83125\n",
      "Iteration: 1500\n",
      "Test loss: \t0.13177050628195414\n",
      "Test acc: \t0.811075\n",
      "Train loss: \t0.11560015764073327\n",
      "Train acc: \t0.84875\n",
      "Iteration: 1600\n",
      "Test loss: \t0.12047811682302496\n",
      "Test acc: \t0.832775\n",
      "Train loss: \t0.10505649146958257\n",
      "Train acc: \t0.87125\n",
      "Iteration: 1700\n",
      "Test loss: \t0.10873323447636524\n",
      "Test acc: \t0.8510125\n",
      "Train loss: \t0.09403564629855567\n",
      "Train acc: \t0.885\n",
      "Iteration: 1800\n",
      "Test loss: \t0.09645844808709687\n",
      "Test acc: \t0.87575\n",
      "Train loss: \t0.08245983459599639\n",
      "Train acc: \t0.91125\n",
      "Iteration: 1900\n",
      "Test loss: \t0.08393671179592133\n",
      "Test acc: \t0.8949375\n",
      "Train loss: \t0.07068553844571018\n",
      "Train acc: \t0.9275\n",
      "Iteration: 2000\n",
      "Test loss: \t0.07182903172005962\n",
      "Test acc: \t0.939025\n",
      "Train loss: \t0.05945048582308887\n",
      "Train acc: \t0.965\n",
      "Iteration: 2100\n",
      "Test loss: \t0.06079594601444186\n",
      "Test acc: \t0.9616875\n",
      "Train loss: \t0.0494227304952156\n",
      "Train acc: \t0.98\n",
      "Iteration: 2200\n",
      "Test loss: \t0.05121002807163171\n",
      "Test acc: \t0.97765\n",
      "Train loss: \t0.04090545559580084\n",
      "Train acc: \t0.9875\n",
      "Iteration: 2300\n",
      "Test loss: \t0.04314442590451972\n",
      "Test acc: \t0.9854\n",
      "Train loss: \t0.033901589746614336\n",
      "Train acc: \t0.99375\n",
      "Iteration: 2400\n",
      "Test loss: \t0.03647614842191203\n",
      "Test acc: \t0.989425\n",
      "Train loss: \t0.028249482832976715\n",
      "Train acc: \t0.99625\n",
      "Iteration: 2500\n",
      "Test loss: \t0.031009517173647715\n",
      "Test acc: \t0.9936\n",
      "Train loss: \t0.023724547167044224\n",
      "Train acc: \t0.99625\n",
      "Iteration: 2600\n",
      "Test loss: \t0.026543790147599555\n",
      "Test acc: \t0.996225\n",
      "Train loss: \t0.020105567429409322\n",
      "Train acc: \t1.0\n",
      "Iteration: 2700\n",
      "Test loss: \t0.02289664153318118\n",
      "Test acc: \t0.997225\n",
      "Train loss: \t0.01720217786651406\n",
      "Train acc: \t1.0\n",
      "Iteration: 2800\n",
      "Test loss: \t0.019911063839369178\n",
      "Test acc: \t0.998125\n",
      "Train loss: \t0.014859940561590132\n",
      "Train acc: \t1.0\n",
      "Iteration: 2900\n",
      "Test loss: \t0.01745618619900322\n",
      "Test acc: \t0.99855\n",
      "Train loss: \t0.012957001714057323\n",
      "Train acc: \t1.0\n",
      "Iteration: 3000\n",
      "Test loss: \t0.015425513022325782\n",
      "Test acc: \t0.9989875\n",
      "Train loss: \t0.011398542241428418\n",
      "Train acc: \t1.0\n",
      "Iteration: 3100\n",
      "Test loss: \t0.013733891609929912\n",
      "Test acc: \t0.999225\n",
      "Train loss: \t0.010111276781368233\n",
      "Train acc: \t1.0\n",
      "Iteration: 3200\n",
      "Test loss: \t0.012314003099724377\n",
      "Test acc: \t0.9994125\n",
      "Train loss: \t0.009038703461357309\n",
      "Train acc: \t1.0\n",
      "Iteration: 3300\n",
      "Test loss: \t0.01111293616992719\n",
      "Test acc: \t0.9995625\n",
      "Train loss: \t0.00813723766745073\n",
      "Train acc: \t1.0\n",
      "Iteration: 3400\n",
      "Test loss: \t0.010089160805792646\n",
      "Test acc: \t0.999625\n",
      "Train loss: \t0.007373165073067658\n",
      "Train acc: \t1.0\n",
      "Iteration: 3500\n",
      "Test loss: \t0.00921001862081393\n",
      "Test acc: \t0.9997\n",
      "Train loss: \t0.006720288291310456\n",
      "Train acc: \t1.0\n",
      "Iteration: 3600\n",
      "Test loss: \t0.00844972207074075\n",
      "Test acc: \t0.9997375\n",
      "Train loss: \t0.0061581328281312955\n",
      "Train acc: \t1.0\n",
      "Iteration: 3700\n",
      "Test loss: \t0.007787796778838052\n",
      "Test acc: \t0.9998125\n",
      "Train loss: \t0.005670590161794564\n",
      "Train acc: \t1.0\n",
      "Iteration: 3800\n",
      "Test loss: \t0.0072078844489738385\n",
      "Test acc: \t0.9998625\n",
      "Train loss: \t0.005244895391305553\n",
      "Train acc: \t1.0\n",
      "Iteration: 3900\n",
      "Test loss: \t0.006696828009857795\n",
      "Test acc: \t0.9998875\n",
      "Train loss: \t0.00487085750343185\n",
      "Train acc: \t1.0\n",
      "Iteration: 4000\n",
      "Test loss: \t0.006243972940155413\n",
      "Test acc: \t0.9998875\n",
      "Train loss: \t0.004540278853407204\n",
      "Train acc: \t1.0\n",
      "Iteration: 4100\n",
      "Test loss: \t0.005840632458088361\n",
      "Test acc: \t0.9999\n",
      "Train loss: \t0.004246515825540142\n",
      "Train acc: \t1.0\n",
      "Iteration: 4200\n",
      "Test loss: \t0.005479676496855446\n",
      "Test acc: \t0.9999\n",
      "Train loss: \t0.0039841447608524585\n",
      "Train acc: \t1.0\n",
      "Iteration: 4300\n",
      "Test loss: \t0.005155214259175616\n",
      "Test acc: \t0.999925\n",
      "Train loss: \t0.0037487065015663007\n",
      "Train acc: \t1.0\n",
      "Iteration: 4400\n",
      "Test loss: \t0.004862347725068435\n",
      "Test acc: \t0.999925\n",
      "Train loss: \t0.0035365098427276786\n",
      "Train acc: \t1.0\n",
      "Iteration: 4500\n",
      "Test loss: \t0.004596979176640518\n",
      "Test acc: \t0.99995\n",
      "Train loss: \t0.003344479320343244\n",
      "Train acc: \t1.0\n",
      "Iteration: 4600\n",
      "Test loss: \t0.004355660037506526\n",
      "Test acc: \t0.99995\n",
      "Train loss: \t0.0031700365460928864\n",
      "Train acc: \t1.0\n",
      "Iteration: 4700\n",
      "Test loss: \t0.004135471469925197\n",
      "Test acc: \t0.99995\n",
      "Train loss: \t0.003011007072536892\n",
      "Train acc: \t1.0\n",
      "Iteration: 4800\n",
      "Test loss: \t0.003933929512101752\n",
      "Test acc: \t0.99995\n",
      "Train loss: \t0.0028655468078492407\n",
      "Train acc: \t1.0\n",
      "Iteration: 4900\n",
      "Test loss: \t0.0037489092813174814\n",
      "Test acc: \t0.99995\n",
      "Train loss: \t0.002732083495034724\n",
      "Train acc: \t1.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "train_iters = 5000\n",
    "print_iters = 100\n",
    "\n",
    "\n",
    "trainX, trainY = generate_data(100, 8)\n",
    "testX, testY = generate_data(10000, 8)\n",
    "print(\"-----------------------------------\")\n",
    "print(trainX[0,:,0])\n",
    "print(\"+\")\n",
    "print(trainX[0,:,1])\n",
    "print(\"=\")\n",
    "print(trainY[0,:,0])\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Shapes\")\n",
    "net = one_layer_rnn(2, 16, 1)\n",
    "result = net.forward_propagation(trainX)\n",
    "net.backprop_through_time(trainY)\n",
    "print(\"X        shape: {}\".format(net.X.shape))\n",
    "print(\"H        shape: {}\".format(net.H.shape))\n",
    "print(\"out      shape: {}\".format(net.out.shape))\n",
    "print(\"d_out    shape: {}\".format(net.d_out.shape))\n",
    "print(\"d_hidden shape: {}\".format(net.d_hidden.shape))\n",
    "print(\"dW2      shape: {}\".format(net.dW2.shape))\n",
    "print(\"dW1      shape: {}\".format(net.dW1.shape))\n",
    "print(\"dU       shape: {}\".format(net.dU.shape))\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "for i in range(train_iters):\n",
    "    if i% print_iters == 0:\n",
    "        result = net.forward_propagation(testX)\n",
    "        print(\"Iteration: {}\".format(i))\n",
    "        print(\"Test loss: \\t{}\".format(mean_square_error(result, testY)))\n",
    "        print(\"Test acc: \\t{}\".format(accuracy(result, testY)))\n",
    "        result = net.forward_propagation(trainX)\n",
    "        print(\"Train loss: \\t{}\".format(mean_square_error(result, trainY)))\n",
    "        print(\"Train acc: \\t{}\".format(accuracy(result, trainY)))\n",
    "    result = net.forward_propagation(trainX)\n",
    "    net.backprop_through_time(trainY)\n",
    "    net.gradient_step(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DL-T1_excercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
