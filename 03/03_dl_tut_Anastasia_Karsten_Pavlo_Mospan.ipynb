{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - DL Tutorial 03\n",
    "\n",
    "Please complete the following notebook and submit your solutions to manuel.milling@informatik.uni-augsburg.de and maurice.gerczuk@informatik.uni-augsburg.de by May 05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student name: Anastasia Karsten, Pavlo Mospan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mnist data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainx shape: (60000, 784)\n",
      "Trainy shape: (60000,)\n",
      "Testx shape:  (10000, 784)\n",
      "Testy shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#numpy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "trainx, trainy, testx, testy = np.load('mnist.npy', allow_pickle=True)\n",
    "print(\"Trainx shape: {}\".format(trainx.shape))\n",
    "print(\"Trainy shape: {}\".format(trainy.shape))\n",
    "print(\"Testx shape:  {}\".format(testx.shape))\n",
    "print(\"Testy shape:  {}\".format(testy.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Implement sigmoid-function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    \"\"\"\n",
    "    X: input: shape (num_samples, num_neurons)\n",
    "    return: element-wise application of sigmoid function\n",
    "    \"\"\"\n",
    "    return(1/(1+np.exp(-X)))\n",
    "\n",
    "# test = np.array([[2,3], [4,5]])\n",
    "# print(sigmoid(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Implement forward propagation for one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcc_one_layer(H, W, b, activation):\n",
    "    \"\"\"\n",
    "    H: input: shape (num_samples, num_neurons_in)\n",
    "    W: weights: shape (num_neurons_in, num_neurons_out)\n",
    "    b: bias: shape (num_neurons_out,)\n",
    "    activation: activation function: python method\n",
    "    return: forward propagation of layer.\n",
    "    \"\"\"\n",
    "    return(activation(np.add(np.matmul(H, W),b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Implement the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    \"\"\"\n",
    "    X: input: shape (num_samples, num_neurons)\n",
    "    return: softmax function applied to neuron axis.\n",
    "    \"\"\"\n",
    "    return(np.exp(X) / np.sum(np.exp(X), axis=0))\n",
    "\n",
    "# test = np.array([[2,3], [4,5]])\n",
    "# print(softmax(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.   Implement the neural network class with weight- and bias-initialization. Note: First initialise all weights, then all biases in order to compare your results for the given random seed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   Implement the full forward propagation for our neural network and the given architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcc:\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_out):\n",
    "        \"\"\"\n",
    "        n_input: number of neurons in input layer: int\n",
    "        n_hidden1: number of neurons in hidden layer 1: int\n",
    "        n_hidden2: number of neurons in hidden layer 2: int\n",
    "        n_out: number of neurons in output layer: int\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(n_input, n_hidden1)\n",
    "        self.W2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "        self.W3 = np.random.randn(n_hidden2, n_out)\n",
    "\n",
    "        # self.b0 = np.random.randn(n_input)\n",
    "        self.b1 = np.random.randn(n_hidden1)\n",
    "        self.b2 = np.random.randn(n_hidden2)\n",
    "        self.b3 = np.random.randn(n_out)\n",
    "        \n",
    "        self.numberOfParams = self.W1.size + self.W2.size + self.W3.size + len(self.b1) + len(self.b2) + len(self.b3)\n",
    "\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        X: input: shape (num_samples, num_pixels)\n",
    "        return: predicition of the neural network\n",
    "        \"\"\"\n",
    "        step1 = fcc_one_layer(X, self.W1, self.b1, sigmoid)\n",
    "        step2 = fcc_one_layer(step1, self.W2, self.b2, sigmoid)\n",
    "        return(fcc_one_layer(step2, self.W3, self.b3, softmax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.   Implement a function for the cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, labels):\n",
    "    \"\"\"\n",
    "    predictions: predicted probabilities for classes: shape (num_samples, num_classes)\n",
    "    labels: correct classes: shape (num_samples,)\n",
    "    return: cross_entropy averaged across all samples \n",
    "    \"\"\"\n",
    "    # return np.sum(-np.log(predictions[range(len(labels))], labels))/len(labels)\n",
    "    \n",
    "    m = labels.shape[0]\n",
    "    log_likelihood = -np.log(predictions[range(m),labels])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.   Implement a function for the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    predictions: predicted probabilities for classes: shape (num_samples, num_classes)\n",
    "    labels: correct classes: shape (num_samples,)\n",
    "    return: accuracy of the predictions\n",
    "    \"\"\"\n",
    "    rights = 0\n",
    "\n",
    "    pr = np.argmax(predictions, axis=1)\n",
    "    for i in range(len(pr)):\n",
    "        if pr[i] == labels[i]:\n",
    "            rights += 1\n",
    "    return rights/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.   Evaluate the loss and the accuracy of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy:  28.691579283356656\n",
      "Accuracy: 0.09853333333333333\n"
     ]
    }
   ],
   "source": [
    "network = fcc(784, 400, 400, 10)\n",
    "predictions = network.forward_propagation(trainx)\n",
    "print(\"Cross-Entropy: \", cross_entropy(predictions, trainy))\n",
    "print(\"Accuracy:\", accuracy(predictions, trainy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.   Which are the parameters we can tune to improve the performance of our network? How many trainable parameters (scalars) does our network have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478410"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can tune parameters of hidden layers (hidden1, hidden2), since they brin complexity to the model. \n",
    "# We could make less / more neurons in each layer.\n",
    "\n",
    "# Trainable parameters : \n",
    "# n_iput * n_hidden1 + b1 + n_hidden1 * n_hidden2 + b2 + n_hidden2 * out + b3 = \n",
    "# = 784 * 400 + 400 + 400 * 400 + 400 + 400 * 10 + 10 = 478,410\n",
    "\n",
    "network.numberOfParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.  Why did we implement two different evaluation metrics of our system (cross-entropy and accuracy)? What are the main differences between the two and why can’t/shouldn’t we use them interchangeably?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and cross-entropy measure different things. Cross-entropy loss is the difference between the predicted value of model and the true value, thus awarding lower loss to predictions which are closer to the class label. \n",
    "<b>A loss function is used to optimize a machine learning algorithm.</b>\n",
    "\n",
    "The accuracy, on the other hand, is a binary true/false for a particular sample. It tends to increase with the decrease in loss. <b>An accuracy metric is used to measure the algorithm’s performance (accuracy) in an interpretable way.</b> Loss is not always a good and trustworthy performance measure, cause if 90% of the samples are “red”, then the model would have good accuracy score if it simply predicts “red” every time.\n",
    "\n",
    "So, cross-entropy is a continuous variable i.e. it’s best when predictions are close to 1 (for true labels) and close to 0 (for false ones) and accuracy is discrete valued.\n",
    "\n",
    "Sometimes loss is getting better while accuracy is getting worse, or vice versa. If the model becomes over-confident in its predictions, a single false prediction will increase the loss unproportionally compared to the (minor) drop in accuracy. An over-confident (overfit) model can have good accuracy but bad loss.\n",
    "\n",
    "The other reason why we cannot use them both inerchangeably is the fact, that accuracy isn’t differentiable so it can’t be used for back-propagation by the learning algorithm and we need a differentiable loss function to act as a good proxy for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
