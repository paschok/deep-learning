{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - DL Tutorial 04: FCNN - BP \n",
    "\n",
    "Please complete the following notebook and submit your solutions to manuel.milling@informatik.uni-augsburg.de and maurice.gerczuk@informatik.uni-augsburg.de till 10 May 14:15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student name: Pavlo Mospan, Anastasia Karsten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions from exercise sheet 3 (class methods below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainx shape: (60000, 784)\n",
      "Trainy shape: (60000,)\n",
      "Testx shape:  (10000, 784)\n",
      "Testy shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#numpy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "trainx, trainy, testx, testy = np.load('../03/mnist.npy', allow_pickle=True)\n",
    "print(\"Trainx shape: {}\".format(trainx.shape))\n",
    "print(\"Trainy shape: {}\".format(trainy.shape))\n",
    "print(\"Testx shape:  {}\".format(testx.shape))\n",
    "print(\"Testy shape:  {}\".format(testy.shape))\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "    #more stable\n",
    "    eps = X.max()\n",
    "    return np.exp(X + eps)/(np.sum(np.exp(X + eps), axis=1).reshape((X.shape[0],1)))\n",
    "\n",
    "def fcc_one_layer(X, W, b, activation):\n",
    "    return activation(np.matmul(X, W) + b)\n",
    "\n",
    "def cross_entropy(pred_logits, y):\n",
    "    num_data_points = pred_logits.shape[0]\n",
    "    correct_logits = pred_logits[np.arange(num_data_points),y]\n",
    "    return np.mean(-np.log(correct_logits))\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    class_predictions = np.argmax(logits, axis=1)\n",
    "    return np.mean(class_predictions == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Implement the error of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_last_layer(H, y):\n",
    "    \"\"\"\n",
    "    :param H: softmax activations of shape (num_examples, num_classes)\n",
    "    :param y: correct labels of shape (num_examples,)\n",
    "    :return: delta of of last layer, i.e. derivative of cross entropy times derivative of softmax\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    gradient = H\n",
    "    gradient[range(m),y] -= 1\n",
    "    gradient = gradient/m\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Implement the derivative of the sigmoid function in terms of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_sigmoid(H):\n",
    "    \"\"\"\n",
    "    :param H: output of the sigmoid function shape (num_examples, num_units)\n",
    "    :return: element-wise derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return H * (1 - H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.   Implement the backpropagation as a class method.\n",
    "4.   Implement the the optimisation step as a class method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcc:\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_out):\n",
    "        #parameters\n",
    "        self.W_i_h1 = np.random.randn(n_input, n_hidden1)\n",
    "        self.b_h1 = np.random.randn(n_hidden1)\n",
    "        self.W_h1_h2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "        self.b_h2 = np.random.randn(n_hidden2)\n",
    "        self.W_h2_o = np.random.randn(n_hidden2, n_out)\n",
    "        self.b_out = np.random.randn(n_out)\n",
    "        #neuron activations and input H^n\n",
    "        self.X = None\n",
    "        self.h1 = None\n",
    "        self.h2 = None\n",
    "        self.out = None\n",
    "        #components of the gradient\n",
    "        self.dW_i_h1 = None\n",
    "        self.db_h1 = None\n",
    "        \n",
    "        self.dW_h1_h2 = None\n",
    "        self.db_h2 = None\n",
    "        \n",
    "        self.dW_h2_o = None\n",
    "        self.db_out = None\n",
    "\n",
    "        n_trainable_bias = self.b_h1.shape[0] + self.b_h2.shape[0] + self.b_out.shape[0]\n",
    "        n_trainable_weights = self.W_i_h1.shape[0] * self.W_i_h1.shape[1] + self.W_h1_h2.shape[0] * self.W_h1_h2.shape[1] + self.W_h2_o.shape[0] * self.W_h2_o.shape[1]\n",
    "        print(\"Number of parameters: {}\".format(n_trainable_bias + n_trainable_weights))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.X = X\n",
    "        self.h1 = fcc_one_layer(X, self.W_i_h1, self.b_h1, sigmoid)\n",
    "        self.h2 = fcc_one_layer(self.h1, self.W_h1_h2, self.b_h2, sigmoid)\n",
    "        self.out = fcc_one_layer(self.h2, self.W_h2_o, self.b_out, softmax)\n",
    "        return self.out\n",
    "\n",
    "    def backprop(self, y):\n",
    "        \"\"\"\n",
    "        :param y: labels, i.e. numbers of correct classes of shape (num_examples,)\n",
    "        \"\"\"\n",
    "        # outer layer (Dims : 400 x 10) + bias (Dims : 10 x 1)\n",
    "        \n",
    "        delta = delta_last_layer(self.out, y)\n",
    "        self.db_out = np.mean(delta) # * self.b_out\n",
    "        self.dW_h2_o = np.matmul(np.transpose (self.h2), delta)\n",
    "             \n",
    "        # hidden2 layer (Dims : 400 x 400 ) + bias (Dims : 400 x 1)\n",
    "        \n",
    "        sigmoid = del_sigmoid(self.h2)\n",
    "        self.db_h2 = np.mean(sigmoid) # * self.b_h2\n",
    "        self.dW_h1_h2 = np.matmul(np.transpose(self.h1), sigmoid)\n",
    "        \n",
    "        # hidden1 layer (Dims : 784 x 400) + bias (Dims : 400 x 1)\n",
    "        \n",
    "        sigmoid2 =  del_sigmoid(self.h1)\n",
    "        self.db_h1 = np.mean(sigmoid2) # * self.b_h1\n",
    "        self.dW_i_h1 = np.matmul(np.transpose(self.X), sigmoid)\n",
    "        \n",
    "    \n",
    "    def gradient_step(self, learning_rate):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning_rate for training\n",
    "        \"\"\"\n",
    "        # input -> hidden1\n",
    "        self.W_i_h1 = self.W_i_h1 - self.dW_i_h1 * learning_rate\n",
    "        self.b_h1 = self.b_h1 - self.db_h1 * learning_rate\n",
    "        \n",
    "        # hidden1 -> hidden2\n",
    "        self.W_h1_h2 = self.W_h1_h2 - self.dW_h1_h2 * learning_rate\n",
    "        self.b_h2 = self.b_h2 - self.db_h2 * learning_rate\n",
    "        \n",
    "        # hidden2 -> out\n",
    "        self.W_h2_o = self.W_h2_o - self.dW_h2_o * learning_rate\n",
    "        self.b_out = self.b_out - self.db_out * learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   Implement the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 478410\n",
      "Iteration : \t 0\n",
      "Train Loss:\t20.01954426412725\n",
      "Train Accuracy:\t0.11021666666666667 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-cd8754eb0605>:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : \t 10\n",
      "Train Loss:\t12.699778525652373\n",
      "Train Accuracy:\t0.09863333333333334 \n",
      "\n",
      "Iteration : \t 20\n",
      "Train Loss:\t9.89477099552828\n",
      "Train Accuracy:\t0.10218333333333333 \n",
      "\n",
      "Iteration : \t 30\n",
      "Train Loss:\t7.779723004971075\n",
      "Train Accuracy:\t0.10218333333333333 \n",
      "\n",
      "Iteration : \t 40\n",
      "Train Loss:\t6.3727033076220865\n",
      "Train Accuracy:\t0.10218333333333333 \n",
      "\n",
      "Iteration : \t 50\n",
      "Train Loss:\t5.527972248463615\n",
      "Train Accuracy:\t0.10218333333333333 \n",
      "\n",
      "Iteration : \t 60\n",
      "Train Loss:\t4.779275364251287\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 70\n",
      "Train Loss:\t4.066664014413192\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 80\n",
      "Train Loss:\t3.450223898681594\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 90\n",
      "Train Loss:\t3.0247587639349685\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 100\n",
      "Train Loss:\t2.7446440925807383\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 110\n",
      "Train Loss:\t2.5426907564509755\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 120\n",
      "Train Loss:\t2.4114366235093003\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 130\n",
      "Train Loss:\t2.3376988718202347\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 140\n",
      "Train Loss:\t2.3088181902055807\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 150\n",
      "Train Loss:\t2.302342069548015\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 160\n",
      "Train Loss:\t2.3014992538694137\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 170\n",
      "Train Loss:\t2.3014658758011834\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 180\n",
      "Train Loss:\t2.3014933438323215\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 190\n",
      "Train Loss:\t2.3015110579584155\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 200\n",
      "Train Loss:\t2.3015212792413426\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 210\n",
      "Train Loss:\t2.3015282841040934\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 220\n",
      "Train Loss:\t2.3015340564886753\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 230\n",
      "Train Loss:\t2.3015393822673214\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 240\n",
      "Train Loss:\t2.3015445639779792\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 250\n",
      "Train Loss:\t2.3015497194459824\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 260\n",
      "Train Loss:\t2.3015548963031773\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 270\n",
      "Train Loss:\t2.3015601152476637\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 280\n",
      "Train Loss:\t2.3015653864809007\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 290\n",
      "Train Loss:\t2.3015707160308607\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 300\n",
      "Train Loss:\t2.3015761082171515\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 310\n",
      "Train Loss:\t2.3015815666224073\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 320\n",
      "Train Loss:\t2.301587094476159\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 330\n",
      "Train Loss:\t2.3015926948040533\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 340\n",
      "Train Loss:\t2.3015983704820866\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 350\n",
      "Train Loss:\t2.3016041242520537\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 360\n",
      "Train Loss:\t2.3016099587212118\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 370\n",
      "Train Loss:\t2.3016158763557444\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 380\n",
      "Train Loss:\t2.3016218794720973\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 390\n",
      "Train Loss:\t2.3016279702279356\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 400\n",
      "Train Loss:\t2.3016341506135145\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 410\n",
      "Train Loss:\t2.301640422443824\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 420\n",
      "Train Loss:\t2.3016467873516575\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 430\n",
      "Train Loss:\t2.3016532467817\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 440\n",
      "Train Loss:\t2.301659801985653\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 450\n",
      "Train Loss:\t2.3016664540184197\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 460\n",
      "Train Loss:\t2.3016732037353265\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 470\n",
      "Train Loss:\t2.301680051790384\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 480\n",
      "Train Loss:\t2.3016869986355393\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 490\n",
      "Train Loss:\t2.3016940445209197\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 500\n",
      "Train Loss:\t2.301701189496001\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 510\n",
      "Train Loss:\t2.301708433411673\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 520\n",
      "Train Loss:\t2.301715775923164\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 530\n",
      "Train Loss:\t2.3017232164937624\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 540\n",
      "Train Loss:\t2.30173075439929\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 550\n",
      "Train Loss:\t2.3017383887332814\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 560\n",
      "Train Loss:\t2.301746118412791\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 570\n",
      "Train Loss:\t2.301753942184806\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 580\n",
      "Train Loss:\t2.3017618586331814\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 590\n",
      "Train Loss:\t2.3017698661860497\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 600\n",
      "Train Loss:\t2.301777963123647\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 610\n",
      "Train Loss:\t2.3017861475865042\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 620\n",
      "Train Loss:\t2.3017944175839453\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 630\n",
      "Train Loss:\t2.3018027710028357\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 640\n",
      "Train Loss:\t2.301811205616529\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 650\n",
      "Train Loss:\t2.301819719093977\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 660\n",
      "Train Loss:\t2.301828309008928\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 670\n",
      "Train Loss:\t2.301836972849186\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 680\n",
      "Train Loss:\t2.3018457080258883\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 690\n",
      "Train Loss:\t2.3018545118827443\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 700\n",
      "Train Loss:\t2.301863381705214\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 710\n",
      "Train Loss:\t2.3018723147295823\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 720\n",
      "Train Loss:\t2.301881308151893\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 730\n",
      "Train Loss:\t2.3018903591367104\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 740\n",
      "Train Loss:\t2.3018994648257003\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 750\n",
      "Train Loss:\t2.3019086223459655\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 760\n",
      "Train Loss:\t2.3019178288181603\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 770\n",
      "Train Loss:\t2.3019270813643193\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 780\n",
      "Train Loss:\t2.301936377115414\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 790\n",
      "Train Loss:\t2.3019457132186045\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 800\n",
      "Train Loss:\t2.301955086844183\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 810\n",
      "Train Loss:\t2.3019644951921907\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 820\n",
      "Train Loss:\t2.301973935498699\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 830\n",
      "Train Loss:\t2.301983405041761\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 840\n",
      "Train Loss:\t2.301992901147003\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 850\n",
      "Train Loss:\t2.302002421192881\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 860\n",
      "Train Loss:\t2.302011962615588\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 870\n",
      "Train Loss:\t2.3020215229135963\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 880\n",
      "Train Loss:\t2.3020310996518747\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 890\n",
      "Train Loss:\t2.30204069046574\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 900\n",
      "Train Loss:\t2.3020502930643807\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 910\n",
      "Train Loss:\t2.3020599052340427\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 920\n",
      "Train Loss:\t2.302069524840883\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 930\n",
      "Train Loss:\t2.302079149833499\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 940\n",
      "Train Loss:\t2.302088778245154\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 950\n",
      "Train Loss:\t2.3020984081956875\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : \t 960\n",
      "Train Loss:\t2.3021080378931336\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 970\n",
      "Train Loss:\t2.3021176656350555\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 980\n",
      "Train Loss:\t2.302127289809603\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 990\n",
      "Train Loss:\t2.3021369088963013\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n",
      "Iteration : \t 1000\n",
      "Train Loss:\t2.302146521466598\n",
      "Train Accuracy:\t0.11236666666666667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "neural_net = fcc(784, 400, 400, 10)\n",
    "\n",
    "for i in range(num_iterations + 1):\n",
    "    logits = neural_net.forward_propagation(trainx)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Iteration : \\t', i)\n",
    "        ce = cross_entropy(logits, trainy)\n",
    "        print(\"Train Loss:\\t{}\".format(ce))\n",
    "        \n",
    "        acc = accuracy(logits, trainy)\n",
    "        print(\"Train Accuracy:\\t{} \\n\".format(acc))\n",
    "        \n",
    "        with open('results_1000_iterations.csv', mode='a') as file:\n",
    "            file_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            file_writer.writerow([i, ce, acc])\n",
    "\n",
    "    neural_net.backprop(trainy)\n",
    "    neural_net.gradient_step(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Bonus: Stochastic Gradient Descent \n",
    "\n",
    "### Answer to the question: Results are obtained so much faster. Like, in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 478410\n",
      "Iteration : \t 0\n",
      "Train Loss:\t28.127218229806303\n",
      "Train Accuracy:\t0.0625 \n",
      "\n",
      "Iteration : \t 100\n",
      "Train Loss:\t5.058496631008965\n",
      "Train Accuracy:\t0.078125 \n",
      "\n",
      "Iteration : \t 200\n",
      "Train Loss:\t2.9614718473165316\n",
      "Train Accuracy:\t0.09375 \n",
      "\n",
      "Iteration : \t 300\n",
      "Train Loss:\t2.5677177934732813\n",
      "Train Accuracy:\t0.046875 \n",
      "\n",
      "Iteration : \t 400\n",
      "Train Loss:\t2.403050325894572\n",
      "Train Accuracy:\t0.125 \n",
      "\n",
      "Iteration : \t 500\n",
      "Train Loss:\t2.7639809759734715\n",
      "Train Accuracy:\t0.125 \n",
      "\n",
      "Iteration : \t 600\n",
      "Train Loss:\t2.462344417162109\n",
      "Train Accuracy:\t0.109375 \n",
      "\n",
      "Iteration : \t 700\n",
      "Train Loss:\t2.4316533825475686\n",
      "Train Accuracy:\t0.203125 \n",
      "\n",
      "Iteration : \t 800\n",
      "Train Loss:\t2.4815518921598794\n",
      "Train Accuracy:\t0.15625 \n",
      "\n",
      "Iteration : \t 900\n",
      "Train Loss:\t2.624125829059529\n",
      "Train Accuracy:\t0.078125 \n",
      "\n",
      "Iteration : \t 1000\n",
      "Train Loss:\t2.4189620561990366\n",
      "Train Accuracy:\t0.125 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "neural_net = fcc(784, 400, 400, 10)\n",
    "\n",
    "for i in range(num_iterations + 1):\n",
    "    \n",
    "    batch = np.random.randint(low=0, high=len(trainy), size=(64,))\n",
    "    batchx = np.empty((64, 784))\n",
    "    batchy = np.empty((64, ))\n",
    "    \n",
    "    for j in range(len(batch)):\n",
    "        batchx[j] = trainx[batch[j]]\n",
    "        batchy[j] = trainy[batch[j]]\n",
    "        \n",
    "    batchx = batchx.astype(int)\n",
    "    batchy = batchy.astype(int)\n",
    "    logits = neural_net.forward_propagation(batchx)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Iteration : \\t', i)\n",
    "        ce = cross_entropy(logits, batchy)\n",
    "        print(\"Train Loss:\\t{}\".format(ce))\n",
    "        acc = accuracy(logits, batchy)\n",
    "        print(\"Train Accuracy:\\t{} \\n\".format(acc))\n",
    "        \n",
    "    neural_net.backprop(batchy)\n",
    "    neural_net.gradient_step(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Bonus: Derivative of the Softmax Function \n",
    "\n",
    "### Check 04_dl_tut_Pavlo_Mospan_Anastasia_Karsten.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
